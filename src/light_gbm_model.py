# coding: utf-8

import  time
import json
import traceback
import pandas as pd
import os
import numpy as np

from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedShuffleSplit  
import lightgbm as lgb

from dataset import load_data
from utility import write_to_file
from utility import save_model,load_model
from conf import STORAGE

def model_select_main(storage,datasetpath,featureheaders,targethearders):

    savefolder = 'estimatelgbm'

    x,y =  load_data(datasetpath,featureheaders,targethearders,nrows= None)
    
    depthlist =[-1]+list(range(3,8,2))
    depthlist= [5] #set 

    minchildlist=[5,15,20,25,30,40] #grid search min_child_sample
#     minchildlist=[60]# set min_child_sample to 60
    leavelist=[31,41,51]
    min_sum_hessian_in_leafs = [0.007,0.001]
#     min_sum_hessian_in_leafs = [0.007]
    feature_fractions=[1,0.9,0.8,0.6,0.5]
#     feature_fractions=[1]

    drop_rates =[0.1,0.2,0.3,0.4]
#     skip_drops =[0.5,0.4,0.6]
    skip_drops =[0.5]
    boost_rounds = [600,800,900,1000,11000,500,1300]
    
    for max_depth in depthlist :
        for num_leaves in leavelist:    
            for min_child_sample in minchildlist:
                for feature_fraction in feature_fractions:
                    for min_sum_hessian_in_leaf in min_sum_hessian_in_leafs:
                        for drop_rate in drop_rates:
                            for skip_drop in skip_drops:
                                for boost_round in boost_rounds:
                                    params = {
                                    'boosting_type': 'dart',
                                    'objective': 'multiclass',
                                    'metric': 'multi_logloss',                
                                    'min_child_samples':min_child_sample,
                                    'num_leaves':num_leaves,
                                    'max_depth':max_depth,
                                    'learning_rate': 0.1,
                                    'num_class':6, 
                                    'num_threads':27,
                                    'max_bin':6,
                                    'lambda_l1':1,
                                    'lambda_l2':0.8,                
                                    'feature_fraction':feature_fraction,
                                    'min_sum_hessian_in_leaf':min_sum_hessian_in_leaf,
                                    'drop_rate':drop_rate,
                                    'skip_drop':skip_drop,
                                    'verbose': 1
                                }
                                    rst =   cross_validation(x, y, 5,params,boost_round)
                                    
                                    rst['datasetpath'] = datasetpath
                                    
                                    txt= json.dumps(rst,indent=4)
                                    summarypath =os.path.join(storage,'model/'+savefolder,str(int(time.time()))+'.model.esimate')
                                    write_to_file(summarypath,txt.encode('utf-8'),mode='wb+')       
       
def cross_validation(x,y,kfolder,params,num_boost_round):
    print(time.asctime(),'cross validation')
    stime = time.time()
    
    num, num_feature = x.shape
    
    # create dataset for lightgbm
    # if you want to re-use data, remember to set free_raw_data=False
    lgb_train = lgb.Dataset(x, y,
                            weight=None, free_raw_data=False)

    
    # generate a feature name
    feature_name = ['feature_' + str(col) for col in range(num_feature)]
    
    print('Start cross validation...')
    # feature_name and categorical_feature
            # specify your configurations as a dict

        
    result = lgb.cv(params,
                    lgb_train,
                    num_boost_round=num_boost_round,
                    nfold=kfolder,                   
                    feature_name=feature_name,                    
                    verbose_eval=-1,
                    show_stdv=True)
    print(params)
    print(result)

#                 print('7th feature name is:', repr(model.feature_name[6]))
    return {'parameter':params,'result':result,'total_sample':num,'total_feautre':num_feature,'boost_round':num_boost_round}
                

def model_build(datasetpath,featureheaders,targethearders):
    """
    choose the selected model
    """
    print(time.asctime(),'loading data start')
    
    x,y =  load_data(datasetpath,featureheaders,targethearders,nrows=None) 
    
    print(time.asctime(),'loading data end','total len',len(x))   

    modelinfo={}
    model = None
    try :
        
        modelinfo['train_start_time']=time.asctime()
        modelinfo={'name':'xgboost','detail':[],'buildtime':time.asctime()}
        
        test_size =0
        
        sample_weight=None  

        # specify your configurations as a dict
        params = {
            'boosting_type': 'dart',
            'objective': 'multiclass',
            'metric': 'multi_logloss',
            'num_leaves': 31,
            'learning_rate': 0.1,
            'min_child_samples':60,                
            'max_depth':5,
            'num_class':6, 
            'num_threads':27,
            'max_bin':6,
            'lambda_l1':1,
            'lambda_l2':0.8,
            'min_sum_hessian_in_leaf':0.007,              
            'verbose': 1
        }
        
        if test_size == 0:
            print('full fit')
            lgb_train = lgb.Dataset(x, y,
                                        weight=None, free_raw_data=False)           
            stime = time.time()
            num_train, num_feature = x.shape
            # generate a feature name
            feature_name = ['feature_' + str(col) for col in range(num_feature)]
            
            print('Start training...')
            # feature_name and categorical_feature
            model = lgb.train(params,
                            lgb_train,
                            num_boost_round=1000,     
#                             learning_rates=lambda iter: 0.1 * (0.99 ** iter),                       
                            feature_name=feature_name)
            
        else:        
            sss = StratifiedShuffleSplit(n_splits=1,test_size=test_size)
            for train, test in sss.split(x, y):
                
                print(time.asctime(),'fit')
                stime = time.time()
                
                X_train = x[train]
                y_train = y[train]
                
                X_test = x[test]
                Y_test = y[test]
                
                num_train, num_feature = X_train.shape
                
                # create dataset for lightgbm
                # if you want to re-use data, remember to set free_raw_data=False
                lgb_train = lgb.Dataset(X_train, y_train,
                                        weight=None, free_raw_data=False)
                lgb_eval = lgb.Dataset(X_test, Y_test, reference=lgb_train,
                                       weight=None, free_raw_data=False)
                

                
                # generate a feature name
                feature_name = ['feature_' + str(col) for col in range(num_feature)]
                
                print('Start training...')
                # feature_name and categorical_feature
                model = lgb.train(params,
                                lgb_train,
                                num_boost_round=2000,
                                valid_sets=lgb_eval,  # eval training data
#                                 learning_rates=lambda iter: 0.1 * (0.99 ** iter),
                                early_stopping_rounds=40,
                                feature_name=feature_name)
                
                x_test_proba = model.predict(X_test)                
                
                loss = log_loss(y[test], x_test_proba,labels=[0,1,2,3,4,5])             
                             
                modelinfo['train_len']=len(train)   
                modelinfo['train_malicious_len']= str(y[train].sum())
                modelinfo['test_len']=len(test)
                modelinfo['test_malicious_len']= str(y[test].sum())           
        
                x_test_class = np.argmax(x_test_proba,axis=1)
                print('x_test_proba',type(x_test_proba))
                print(loss)
                
                print(pd.crosstab(y[test], x_test_class, rownames=['Actual Species'], colnames=['Predicted Species']))
    
                print(time.asctime(),'sample weight',sample_weight)
    
                result = {'loss': loss,
                  'costtime':       time.time()-stime}
            
                modelinfo['detail'].append(result)    

                samples,probas = x_test_proba.shape
                
                print('shape of x_test_proba.shape',x_test_proba.shape)
                
                predict_save_path = os.path.join(STORAGE,'train_predict.lightgbm.csv')
                
                write_to_file(predict_save_path,'',mode='w+')
                
                txt='{},{},{},{},{},{},{},{}\n'.format('file_id','prob0','prob1','prob2','prob3','prob4','prob5','label')
                
                write_to_file(predict_save_path,txt,mode='w+')
                
                
                for i in range(samples):
                    attr=[]
                    attr+=[str(p) for p in x_test_proba[i]]
                    attr+=[str(y[test][i])]
                    txt=','.join(attr)
                    write_to_file(predict_save_path,txt+'\n',mode='a+')

        modelinfo['name']='lightgbm'
        modelinfo['attibutes_num']=x.shape[1]
        modelinfo['dataset_path']=datasetpath

        modelinfo['sample_weight']=sample_weight        
       
        modelinfo['train_end_time']=time.asctime()
        modelinfo['test_size']=test_size
    
    except Exception as e:
        traceback.print_exc()
        print('excepion',str(e))
    return 'lightgbm',model,modelinfo

def model_build_main(storage,datasetpath,featureheaders,targethearders):
    
    name,clf,modelinfo = model_build(datasetpath,featureheaders,targethearders)
    
    summarypath = os.path.join(storage,'model/lightgbm.model.esimate')
    modelsavepath=os.path.join(storage,'model/lightgbm.model')
    modelinfosavepath = os.path.join(storage,'model/lightgbm.modelinfo')
    
    txt=json.dumps(modelinfo,indent=4)
    
    write_to_file(modelinfosavepath, txt.encode('utf-8'), mode='wb+')
    
    save_model(clf,modelsavepath)
    
    print('model summary:' )
    
    print('save model summary->',summarypath )
    write_to_file(summarypath,txt.encode('utf-8'),mode='wb+')     
    
def main():

    feature_save_path = os.path.join(STORAGE,'data/merge.basic.func1.rtnvalue.func23.rtnapi.apitype.csv')
    
    rtn_tfidf_save_path = os.path.join(STORAGE,'rtnvalue_tfidf/webpage.tfidf.model')    
    func1_tfidf_save_path = os.path.join(STORAGE,'apicall_tfidf_1/webpage.tfidf.model')    
    func23_tfidf_save_path = os.path.join(STORAGE,'apicall_tfidf/webpage.tfidf.model')    
    rtnfunc_tfidf_save_path = os.path.join(STORAGE,'rtn_apicall_tfidf/webpage.tfidf.model')
        
    featureheaders = [
                                     
                    ]
    
    basicfeature = [
                      'threadnum',
                      'totalapicall',
                      'maxapicall',
                      'minapicall',
                      'meanapicallperthread'                  
                    ]
    
    featureheaders+=basicfeature
    
    model = load_model(func1_tfidf_save_path)       
    featureheaders += model.get_feature_names()
       
    model = load_model(rtn_tfidf_save_path)       
    featureheaders += model.get_feature_names()
  
    model = load_model(func23_tfidf_save_path)      
    featureheaders += model.get_feature_names()    
        
    model = load_model(rtnfunc_tfidf_save_path)  
    featureheaders += model.get_feature_names()
    
    apitype_save_path = os.path.join(STORAGE,'api_type_tfidf_5/webpage.tfidf.model')
    model = load_model(apitype_save_path)  
    featureheaders += model.get_feature_names()
 
    print(time.asctime(),'total header len',len(featureheaders))
    
    targethearders=['label_x']
    
    # set ModelSelect to True for parameter tuning
    ModelSelect = False
    
    if ModelSelect:    
        model_select_main(STORAGE,feature_save_path,featureheaders,targethearders)
    else:    
        model_build_main(STORAGE,
                         feature_save_path,
                         featureheaders,
                         targethearders)

if __name__ == '__main__':
    
    main()
    
    


